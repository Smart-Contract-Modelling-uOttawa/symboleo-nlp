{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sys\n",
    "import spacy\n",
    "\n",
    "# Load NLP\n",
    "#sys.path.append('../')\n",
    "\n",
    "with open('./nlp/nlp.pickle', 'rb') as f:\n",
    "    nlp = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%store nlp\n",
    "\n",
    "%store -r  nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.src.operations.dependencies import Dependencies\n",
    "from app.src.operations.input_converter_builder import InputConverterBuilder\n",
    "from app.classes.other.user_input import UserInput, NodeType\n",
    "\n",
    "deps = Dependencies(nlp)\n",
    "input_converter = InputConverterBuilder.build(deps)\n",
    "\n",
    "node_list = [\n",
    "    UserInput(NodeType.ROOT),\n",
    "    UserInput(NodeType.BEFORE),\n",
    "    UserInput(NodeType.TIMEPOINT),\n",
    "    UserInput(NodeType.DOMAIN_TIMEPOINT, 'evt_delivered.delDueDate')\n",
    "],\n",
    "\n",
    "res = input_converter.convert(node_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "d = defaultdict(lambda:3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = None\n",
    "ff = MySub2(x)\n",
    "\n",
    "ff.print_me()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.src.nlp.subject_extractor import SubjectExtractor\n",
    "\n",
    "se = SubjectExtractor(nlp)\n",
    "\n",
    "subj_str = 'the buyer'\n",
    "subj = se.extract(subj_str)\n",
    "\n",
    "subj.print_me()\n",
    "\n",
    "subj.to_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_str = 'the old man'\n",
    "subj = se.extract('the old man')\n",
    "\n",
    "doc = nlp(subj_str)\n",
    "\n",
    "heads = [x for x in doc if x.dep_ == 'ROOT']\n",
    "\n",
    "print(len(heads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.src.nlp.verb.verb_extractor_builder import VerbExtractorBuilder\n",
    "\n",
    "verb_extractor = VerbExtractorBuilder.build(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.src.helpers.sentence_summarizer import SentenceSummarizer\n",
    "\n",
    "summarizer = SentenceSummarizer(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 'any legal proceedings'\n",
    "\n",
    "summarizer.summarize(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'paid late'\n",
    "\n",
    "summarizer.summarize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.src.nlp.verb_extractor import VerbExtractor\n",
    "\n",
    "verb_extractor = VerbExtractor(nlp)\n",
    "\n",
    "verb = verb_extractor.extract('paid_late')\n",
    "\n",
    "\n",
    "verb.print_me()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can probably use a matcher here\n",
    "# And also the noun chunks or noun phrases\n",
    "\n",
    "# What were looking for: NP - V(linking) - NP/ADJ\n",
    "## V(linking): Make a list of lemmas\n",
    "## NP/ADJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_linking_verb_lemmas = ['be', 'become', 'seem']\n",
    "sense_lemmas = ['look', 'appear', 'sound', 'taste', 'smell', 'feel']\n",
    "\n",
    "lemma_list = main_linking_verb_lemmas + sense_lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(s)\n",
    "\n",
    "for nc in doc.noun_chunks:\n",
    "    print(nc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = list(doc.sents)[0]\n",
    "print(sent._.parse_string)\n",
    "\n",
    "cs = list(sent._.children)\n",
    "\n",
    "if cs[0]._.labels[0] == 'NP':\n",
    "    np = cs[0].text\n",
    "    print('NP:', np)\n",
    "\n",
    "if cs[1]._.labels[0] == 'VP':\n",
    "    cs2 = list(cs[1]._.children)\n",
    "    verb = cs2[0][0]\n",
    "\n",
    "    if verb.pos_ in ['VERB', 'AUX']:\n",
    "        if verb.lemma_ in lemma_list:\n",
    "            verb_text = verb.text\n",
    "            print('VERB:', verb_text)\n",
    "\n",
    "    pred = cs2[1]\n",
    "    # \n",
    "    pred_labels = list(pred._.labels)\n",
    "    if len(pred_labels) > 0:\n",
    "        if pred_labels[0] in ['ADJP', 'NP']:\n",
    "            pred = pred.text\n",
    "            print('PRED:', pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in sent._.children:\n",
    "    print(x._.labels, x.text)\n",
    "    for xx in x._.constituents:\n",
    "        print('--', xx._.labels)\n",
    "    # for xx in x._.children:\n",
    "    #     print('--', xx._.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher, Matcher\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "main_linking_verb_lemmas = ['be', 'become', 'seem']\n",
    "sense_lemmas = ['look', 'appear', 'sound', 'taste', 'smell', 'feel']\n",
    "# other_linking_lemmas = ['grow, turn, remain, prove, act, stay, get']\n",
    "\n",
    "# Cant use the matcher... We need to know about noun phrases\n",
    "## Need something custom\n",
    "\n",
    "\n",
    "linking_verb_patterns = [\n",
    "    [\n",
    "        { \"\" }\n",
    "    ]\n",
    "]\n",
    "\n",
    "# separate ones for contract event, ob event, power event, etc?\n",
    "domain_event_patterns = [\n",
    "    [{\"LOWER\": {'IN': ['contract']}, \"POS\": \"NOUN\" } , {\"POS\": \"VERB\", \"LEMMA\": {\"IN\": contract_verbs}} ],\n",
    "    [{\"LOWER\": {\"IN\": contract_event_nouns}, \"POS\": \"NOUN\"}, {\"POS\": {\"IN\": [\"VERB\", \"AUX\"]}, \"OP\": \"+\"}, {\"LOWER\": \"not\", \"OP\": \"?\"}, {\"LEMMA\": {\"IN\": contract_event_verbs}, \"OP\": \"?\"}],\n",
    "    [{\"POS\": \"NOUN\", \"LEMMA\": {\"IN\": contract_verb_nouns}}, {\"LOWER\": \"of\"}, {\"LOWER\": \"the\"}, {\"LOWER\": {'IN': ['contract']}, \"POS\": \"NOUN\" }]\n",
    "]\n",
    "\n",
    "matcher.add(\"DOMAIN_EVENT\", domain_event_patterns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symboleo Generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.src.helpers.template_getter import get_template\n",
    "\n",
    "contract = get_template('sample_t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for dk in contract.domain_model.events:\n",
    "    domain_obj = contract.domain_model.events[dk]\n",
    "    date_props = [x for x in domain_obj.props if x.type == 'Date']\n",
    "\n",
    "    for dp in date_props:\n",
    "        next_val = f'{domain_obj.name}.{dp.key}'\n",
    "        results.append(next_val)\n",
    "\n",
    "print(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "decs = contract.contract_spec.declarations\n",
    "\n",
    "for dk in decs:\n",
    "    print(dk)\n",
    "    domain_obj = decs[dk]\n",
    "    # date_props = [x for x in domain_obj.props if x.type == 'Date']\n",
    "\n",
    "    # for dp in date_props:\n",
    "    #     next_val = f'{domain_obj.name}.{dp.key}'\n",
    "    #     results.append(next_val)\n",
    "\n",
    "print(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symboleo_spec = contract_spec.to_sym()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = 'sample'\n",
    "FILEPATH = f'./app/templates/sample/t/{FILENAME}.txt'\n",
    "\n",
    "f = open(FILEPATH, 'w')\n",
    "f.write(symboleo_spec)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.templates.meat_sale.nl_template import nl_template\n",
    "\n",
    "obs = nl_template['obligations']\n",
    "\n",
    "print('OBLIGATIONS')\n",
    "for ob in obs:\n",
    "    print(f'{ob}: {obs[ob]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.templates.meat_sale.symboleo import get_template\n",
    "\n",
    "contract_template = get_template()\n",
    "\n",
    "sym_template = contract_template.to_sym()\n",
    "\n",
    "sym_template_path = './app/templates/meat_sale/symboleo/symboleo_spec.txt'\n",
    "with open(sym_template_path, 'w') as f:\n",
    "    f.write(sym_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = 'deliver the goods within thirty days'\n",
    "\n",
    "summarizer.summarize(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.classes.spec.p_atoms import PAtomPredicate\n",
    "from app.src.old.graph.graph_builder import GraphBuilder\n",
    "from app.src.old.graph.graph_visualizer import GraphVisualizer\n",
    "from app.classes.graph.digraph import Digraph\n",
    "\n",
    "graph_builder = GraphBuilder()\n",
    "graph_visualizer = GraphVisualizer()\n",
    "digraph = graph_builder.build(PAtomPredicate)\n",
    "gv = graph_visualizer.create_viz(digraph.nodes)\n",
    "gv.show('nx.html')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workbench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = 'I arrived within 2 weeks of the project being completed.'\n",
    "\n",
    "doc = nlp(val)\n",
    "\n",
    "summarizer.summarize(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the presence of the contract\n",
    "from nltk.corpus import wordnet as wn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = wn.synsets('contract')\n",
    "\n",
    "contract_synset = wn.synset('contract.n.01')\n",
    "\n",
    "val = 'before the agreement terminates'\n",
    "doc = nlp(val)\n",
    "\n",
    "# Look for a noun that suggests presence of contract\n",
    "## Noun chunk or Noun?\n",
    "## What if it is qualified?\n",
    "nouns = [x for x in doc if x.pos_ == 'NOUN']\n",
    "\n",
    "\n",
    "noun_scores = []\n",
    "\n",
    "for n in nouns:\n",
    "    print(n.text)\n",
    "    n_ss = wn.synsets(n.text, pos=wn.NOUN)\n",
    "    \n",
    "    nd = max([contract_synset.wup_similarity(ns) for ns in n_ss])\n",
    "    noun_scores.append((n, nd))\n",
    "    #print(n_ss)\n",
    "\n",
    "top_ns = max(noun_scores, key=lambda x: x[1])\n",
    "print(top_ns)\n",
    "\n",
    "if top_ns[1] < 0.7:\n",
    "    print('failed threshold')\n",
    "\n",
    "# Look for dependence on event\n",
    "## Case 1: \n",
    "\n",
    "\n",
    "# Contract event\n",
    "\n",
    "\n",
    "# for syn in ss:\n",
    "#     print(syn.name())\n",
    "#     print(syn.definition())\n",
    "#     print('\\n')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contract event\n",
    "- presence of contract\n",
    "- something happening to it - narrow set of verbs\n",
    "\n",
    "examples\n",
    "- termination of contract\n",
    "- contract terminate\n",
    "- the contract terminates\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('sym-env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c5aa13c5662b7cd49c2a6a4cb6c712880a857c98a831bc049e568d94e5223a76"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
